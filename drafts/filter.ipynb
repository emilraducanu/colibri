{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "DistilBert is a smaller version of BERT that is much faster and cheaper.\n",
                "\n",
                "From the paper,\n",
                "\n",
                ">\"we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster\"\n",
                "\n",
                "DistilBert Paper: https://arxiv.org/abs/1910.01108v4"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ds",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
