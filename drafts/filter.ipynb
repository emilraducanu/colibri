{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "DistilBert is a smaller version of BERT that is much faster and cheaper.\n",
                "\n",
                "From the paper,\n",
                "\n",
                ">\"we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster\"\n",
                "\n",
                "DistilBert Paper: https://arxiv.org/abs/1910.01108v4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "from transformers import DistilBertTokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_metadata_scholar(query: str):\n",
                "    \"\"\"Quick description\n",
                "\n",
                "    Long description\n",
                "\n",
                "    Parameters:\n",
                "    query (str):\n",
                "\n",
                "    Returns:\n",
                "\n",
                "    \"\"\"\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Raw database contains 1007 entries (217 included MA and 790 excluded MA), stored into 'raw_data' variable.\n",
                        "151 rows removed because of empty DOIs. Cannot check the uniqueness.\n",
                        "0 rows removed because of empty titles. Cannot be processed by the ML model.\n",
                        "54 rows removed because of empty abstracts. Cannot be processed by the ML model.\n",
                        "8 DOI duplicates and title duplicates removed.\n",
                        "Cleaned database contains 794 entries (212 included MA and 582 excluded MA), stored into 'train_set' variable.\n",
                        "Training set stored into 'train_set' variable and ready to be used.\n",
                        "Summary with the 20 first lines:\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>Screening</th>\n",
                            "      <th>Title</th>\n",
                            "      <th>Abstract</th>\n",
                            "      <th>Keywords</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>Responses of a rice-wheat rotation agroecosyst...</td>\n",
                            "      <td>Climate change is likely to affect agroecosyst...</td>\n",
                            "      <td></td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>The number of cycles of neoadjuvant chemothera...</td>\n",
                            "      <td>Objective No consensus exists on the number of...</td>\n",
                            "      <td>High-grade serous ovarian cancer (HG-SOC); CA-...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>Carbon cycling in temperate grassland under el...</td>\n",
                            "      <td>An increase in mean soil surface temperature h...</td>\n",
                            "      <td>CO; (2); elevated temperature; grassland; heat...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>included</td>\n",
                            "      <td>How can straw incorporation management impact ...</td>\n",
                            "      <td>Straw incorporation (SI) is a common practice ...</td>\n",
                            "      <td>China; Meta analysis; Soil carbon (C) sequestr...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>Comparison of prescribing criteria to evaluate...</td>\n",
                            "      <td>Because inappropriate prescribing is prevalent...</td>\n",
                            "      <td></td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>included</td>\n",
                            "      <td>Responses of microbial biomass carbon and nitr...</td>\n",
                            "      <td>Soil microbes play important roles in regulati...</td>\n",
                            "      <td>Microbial biomass carbon; Microbial biomass ni...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>Are active organic matter fractions suitable i...</td>\n",
                            "      <td>Active fractions of organic matter have been p...</td>\n",
                            "      <td>Active fractions of organic matter; microbial ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>included</td>\n",
                            "      <td>Long-term nitrogen fertilization decreases bac...</td>\n",
                            "      <td>Long-term elevated nitrogen (N) input from ant...</td>\n",
                            "      <td>Actinobacteria; agro-ecosystems; bacterial div...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>Comt and mthfr polymorphisms interaction on co...</td>\n",
                            "      <td>The investigation of the catechol-O-methyltran...</td>\n",
                            "      <td></td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>Global-scale pattern of peatland sphagnum grow...</td>\n",
                            "      <td>High-latitude peatlands contain about one thir...</td>\n",
                            "      <td></td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>10</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>Effects of long-term litter manipulation on so...</td>\n",
                            "      <td>Changes in above-ground litterfall can influen...</td>\n",
                            "      <td></td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>11</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>Depression increases the risk of death indepen...</td>\n",
                            "      <td>BACKGROUNDHow much the association between dep...</td>\n",
                            "      <td>depressive symptoms; mortality; cardiovascular...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>12</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>Pairwise comparison of soil organic particle-s...</td>\n",
                            "      <td>Conversion of native vegetation into fast-grow...</td>\n",
                            "      <td></td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>13</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>Sequence recombination improves target specifi...</td>\n",
                            "      <td>Stability of the collagen triple helix is larg...</td>\n",
                            "      <td></td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>14</th>\n",
                            "      <td>included</td>\n",
                            "      <td>Grazing intensity significantly affects belowg...</td>\n",
                            "      <td>Livestock grazing activities potentially alter...</td>\n",
                            "      <td>carbon sequestration; CO2 emission; heavy graz...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>15</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>General health, sense of coherence and coping ...</td>\n",
                            "      <td>Aim. The assessment of changes in the general ...</td>\n",
                            "      <td>ADHD; the functioning of parents; workshops fo...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>16</th>\n",
                            "      <td>included</td>\n",
                            "      <td>A global synthesis of below-ground carbon resp...</td>\n",
                            "      <td>AimClimate change, especially the wider occurr...</td>\n",
                            "      <td>Below-ground; DOC; insect; meta-analysis; path...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>17</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>Effect of different agricultural practices on ...</td>\n",
                            "      <td>Agricultural practices, particularly land use,...</td>\n",
                            "      <td>global warming potential; greenhouses gas emis...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>18</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>Efficacy of topical recombinant human platelet...</td>\n",
                            "      <td>Objective. Recombinant human platelet-derived ...</td>\n",
                            "      <td></td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19</th>\n",
                            "      <td>excluded</td>\n",
                            "      <td>Response of soil respiration to grazing in an ...</td>\n",
                            "      <td>Alpine meadows are one major type of pasturela...</td>\n",
                            "      <td></td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   Screening                                              Title  \\\n",
                            "0   excluded  Responses of a rice-wheat rotation agroecosyst...   \n",
                            "1   excluded  The number of cycles of neoadjuvant chemothera...   \n",
                            "2   excluded  Carbon cycling in temperate grassland under el...   \n",
                            "3   included  How can straw incorporation management impact ...   \n",
                            "4   excluded  Comparison of prescribing criteria to evaluate...   \n",
                            "5   included  Responses of microbial biomass carbon and nitr...   \n",
                            "6   excluded  Are active organic matter fractions suitable i...   \n",
                            "7   included  Long-term nitrogen fertilization decreases bac...   \n",
                            "8   excluded  Comt and mthfr polymorphisms interaction on co...   \n",
                            "9   excluded  Global-scale pattern of peatland sphagnum grow...   \n",
                            "10  excluded  Effects of long-term litter manipulation on so...   \n",
                            "11  excluded  Depression increases the risk of death indepen...   \n",
                            "12  excluded  Pairwise comparison of soil organic particle-s...   \n",
                            "13  excluded  Sequence recombination improves target specifi...   \n",
                            "14  included  Grazing intensity significantly affects belowg...   \n",
                            "15  excluded  General health, sense of coherence and coping ...   \n",
                            "16  included  A global synthesis of below-ground carbon resp...   \n",
                            "17  excluded  Effect of different agricultural practices on ...   \n",
                            "18  excluded  Efficacy of topical recombinant human platelet...   \n",
                            "19  excluded  Response of soil respiration to grazing in an ...   \n",
                            "\n",
                            "                                             Abstract  \\\n",
                            "0   Climate change is likely to affect agroecosyst...   \n",
                            "1   Objective No consensus exists on the number of...   \n",
                            "2   An increase in mean soil surface temperature h...   \n",
                            "3   Straw incorporation (SI) is a common practice ...   \n",
                            "4   Because inappropriate prescribing is prevalent...   \n",
                            "5   Soil microbes play important roles in regulati...   \n",
                            "6   Active fractions of organic matter have been p...   \n",
                            "7   Long-term elevated nitrogen (N) input from ant...   \n",
                            "8   The investigation of the catechol-O-methyltran...   \n",
                            "9   High-latitude peatlands contain about one thir...   \n",
                            "10  Changes in above-ground litterfall can influen...   \n",
                            "11  BACKGROUNDHow much the association between dep...   \n",
                            "12  Conversion of native vegetation into fast-grow...   \n",
                            "13  Stability of the collagen triple helix is larg...   \n",
                            "14  Livestock grazing activities potentially alter...   \n",
                            "15  Aim. The assessment of changes in the general ...   \n",
                            "16  AimClimate change, especially the wider occurr...   \n",
                            "17  Agricultural practices, particularly land use,...   \n",
                            "18  Objective. Recombinant human platelet-derived ...   \n",
                            "19  Alpine meadows are one major type of pasturela...   \n",
                            "\n",
                            "                                             Keywords  \n",
                            "0                                                      \n",
                            "1   High-grade serous ovarian cancer (HG-SOC); CA-...  \n",
                            "2   CO; (2); elevated temperature; grassland; heat...  \n",
                            "3   China; Meta analysis; Soil carbon (C) sequestr...  \n",
                            "4                                                      \n",
                            "5   Microbial biomass carbon; Microbial biomass ni...  \n",
                            "6   Active fractions of organic matter; microbial ...  \n",
                            "7   Actinobacteria; agro-ecosystems; bacterial div...  \n",
                            "8                                                      \n",
                            "9                                                      \n",
                            "10                                                     \n",
                            "11  depressive symptoms; mortality; cardiovascular...  \n",
                            "12                                                     \n",
                            "13                                                     \n",
                            "14  carbon sequestration; CO2 emission; heavy graz...  \n",
                            "15  ADHD; the functioning of parents; workshops fo...  \n",
                            "16  Below-ground; DOC; insect; meta-analysis; path...  \n",
                            "17  global warming potential; greenhouses gas emis...  \n",
                            "18                                                     \n",
                            "19                                                     "
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# ------- LOADING DATA INTO A DATAFRAME -------\n",
                "\n",
                "\n",
                "# Loading database created by D. Beillouin et al.\n",
                "xlsx_file = pd.ExcelFile(\"/home/er/Documents/Cirad/colibri/data/classification_trainset/classification_trainset.xlsx\")\n",
                "\n",
                "# Adding label \"excluded\" or \"included\" for each MA\n",
                "df_incl = xlsx_file.parse(\"retained_meta-analyses\")\n",
                "df_excl = xlsx_file.parse(\"non_retained_meta-analyses\")\n",
                "df_incl[\"Screening\"] = \"included\"\n",
                "df_excl[\"Screening\"] = \"excluded\"\n",
                "\n",
                "# Keeping only useful attributes\n",
                "attributes_to_keep_incl = [\n",
                "    \"Screening\",\n",
                "    \"link\",\n",
                "    \"Article Title\",\n",
                "    \"Abstract\",\n",
                "    \"Keywords\",\n",
                "]\n",
                "attributes_to_keep_excl = [\n",
                "    \"Screening\",\n",
                "    \"lien pour accès\",\n",
                "    \"title\",\n",
                "    \"Abstract\",\n",
                "    \"Keywords\",\n",
                "]\n",
                "df_incl = df_incl[attributes_to_keep_incl]\n",
                "df_excl = df_excl[attributes_to_keep_excl]\n",
                "\n",
                "# Standardising columns names\n",
                "new_column_names_incl = {\"Article Title\": \"Title\", \"link\": \"DOI\"}\n",
                "new_column_names_excl = {\"title\": \"Title\", \"lien pour accès\": \"DOI\"}\n",
                "df_incl = df_incl.rename(columns=new_column_names_incl)\n",
                "df_excl = df_excl.rename(columns=new_column_names_excl)\n",
                "\n",
                "# Merging exluded and included MA into single dataframe\n",
                "raw_data = pd.concat([df_incl, df_excl], ignore_index=True)\n",
                "raw_data = raw_data.fillna(\"\")\n",
                "\n",
                "size_1 = len(df_incl)\n",
                "size_2 = len(df_excl)\n",
                "size_3 = size_1 + size_2\n",
                "print(\n",
                "    f\"Raw database contains {size_3} entries ({size_1} included MA and {size_2} excluded MA), stored into 'raw_data' variable.\"\n",
                ")\n",
                "\n",
                "# ------- CLEANING -------\n",
                "\n",
                "\n",
                "# Function to get DOIs from URLs\n",
                "def extract_doi(url):\n",
                "    if str(url).startswith(\"https://doi.org/\"):\n",
                "        return str(url)[len(\"https://doi.org/\") :]\n",
                "    else:\n",
                "        return None\n",
                "\n",
                "\n",
                "# Extracting DOIs from URLs\n",
                "raw_data[\"DOI\"] = raw_data[\"DOI\"].apply(extract_doi)\n",
                "\n",
                "# Removing empty DOIs rows\n",
                "raw_data = raw_data.dropna(subset=[\"DOI\"])\n",
                "size_4 = len(raw_data)\n",
                "size_5 = size_3 - size_4\n",
                "print(f\"{size_5} rows removed because of empty DOIs. Cannot check the uniqueness.\")\n",
                "\n",
                "# Removing empty titles rows\n",
                "raw_data[\"Title\"] = raw_data[\"Title\"].replace(\"\", np.nan)\n",
                "raw_data = raw_data.dropna(subset=[\"Title\"])\n",
                "size_6 = len(raw_data)\n",
                "size_7 = size_4 - size_6\n",
                "print(\n",
                "    f\"{size_7} rows removed because of empty titles. Cannot be processed by the ML model.\"\n",
                ")\n",
                "\n",
                "# Removing empty abstracts rows\n",
                "raw_data[\"Abstract\"] = raw_data[\"Abstract\"].replace(\"\", np.nan)\n",
                "raw_data = raw_data.dropna(subset=[\"Abstract\"])\n",
                "size_8 = len(raw_data)\n",
                "size_9 = size_6 - size_8\n",
                "print(\n",
                "    f\"{size_9} rows removed because of empty abstracts. Cannot be processed by the ML model.\"\n",
                ")\n",
                "\n",
                "# Removing DOIs duplicates and titles duplicates\n",
                "raw_data = raw_data.drop_duplicates(subset=[\"DOI\"], keep=\"first\")\n",
                "raw_data = raw_data.drop_duplicates(subset=\"Title\", keep=\"first\")\n",
                "size_10 = len(raw_data)\n",
                "size_11 = size_8 - size_10\n",
                "print(f\"{size_11} DOI duplicates and title duplicates removed.\")\n",
                "\n",
                "# Droping column 'DOI' now we have unique values. No needed for the ML model\n",
                "train_set = raw_data.drop(columns=[\"DOI\"])\n",
                "\n",
                "size_12 = train_set[\"Screening\"].value_counts()\n",
                "size_incl = size_12.loc[\"included\"]\n",
                "size_excl = size_12.loc[\"excluded\"]\n",
                "print(\n",
                "    f\"Cleaned database contains {size_10} entries ({size_incl} included MA and {size_excl} excluded MA), stored into 'train_set' variable.\"\n",
                ")\n",
                "\n",
                "# Shuffling and re-indexing\n",
                "train_set = train_set.astype(str)\n",
                "train_set = train_set.sample(frac=1)\n",
                "train_set = train_set.reset_index(drop=True)\n",
                "\n",
                "train_set.to_pickle(\"/home/er/Documents/Cirad/colibri/data/classification_trainset/classification_trainset.pkl\")\n",
                "\n",
                "print(\"Training set stored into 'train_set' variable and ready to be used.\")\n",
                "print(\"Summary with the 20 first lines:\")\n",
                "train_set.head(20)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
                        "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
                    ]
                },
                {
                    "ename": "ValueError",
                    "evalue": "Expected input batch_size (16) to match target batch_size (0).",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[8], line 169\u001b[0m\n\u001b[1;32m    159\u001b[0m concatenated_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m    160\u001b[0m     [\n\u001b[1;32m    161\u001b[0m         title_outputs\u001b[39m.\u001b[39mlast_hidden_state[:, \u001b[39m0\u001b[39m, :],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    168\u001b[0m logits \u001b[39m=\u001b[39m classification_head(concatenated_output)\n\u001b[0;32m--> 169\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, batch_labels)\n\u001b[1;32m    170\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    171\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
                        "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
                        "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
                        "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
                        "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (16) to match target batch_size (0)."
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from transformers import DistilBertTokenizer, DistilBertModel\n",
                "from sklearn.model_selection import train_test_split\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "\n",
                "# Load pre-trained DistilBERT model and tokenizer\n",
                "model_name = \"distilbert-base-uncased\"\n",
                "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
                "model = DistilBertModel.from_pretrained(model_name)\n",
                "\n",
                "# Convert 'Title', 'Abstract', and 'Keywords' columns to lists of strings\n",
                "titles = train_set[\"Title\"].tolist()\n",
                "abstracts = train_set[\"Abstract\"].tolist()\n",
                "keywords = train_set[\"Keywords\"].tolist()\n",
                "\n",
                "# Tokenize and encode the titles, abstracts, and keywords\n",
                "title_inputs = tokenizer(\n",
                "    titles, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\"\n",
                ")\n",
                "abstract_inputs = tokenizer(\n",
                "    abstracts,\n",
                "    padding=\"max_length\",\n",
                "    truncation=True,\n",
                "    max_length=128,\n",
                "    return_tensors=\"pt\",\n",
                ")\n",
                "keywords_inputs = tokenizer(\n",
                "    keywords, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\"\n",
                ")\n",
                "\n",
                "# Prepare labels (assuming 'Screening' column has 'included' and 'excluded' values)\n",
                "labels = torch.tensor(train_set[\"Screening\"].map({\"included\": 1, \"excluded\": 0}).values)\n",
                "\n",
                "# Create attention masks for each input\n",
                "title_attention_mask = title_inputs[\"attention_mask\"]\n",
                "abstract_attention_mask = abstract_inputs[\"attention_mask\"]\n",
                "keywords_attention_mask = keywords_inputs[\"attention_mask\"]\n",
                "\n",
                "# Split data into training and validation sets\n",
                "(\n",
                "    train_title_inputs,\n",
                "    val_title_inputs,\n",
                "    train_title_attention_mask,\n",
                "    val_title_attention_mask,\n",
                "    train_labels,\n",
                "    val_labels,\n",
                ") = train_test_split(\n",
                "    title_inputs[\"input_ids\"],\n",
                "    title_attention_mask,\n",
                "    labels,\n",
                "    test_size=0.2,\n",
                "    random_state=42,\n",
                ")\n",
                "\n",
                "(\n",
                "    train_abstract_inputs,\n",
                "    val_abstract_inputs,\n",
                "    train_abstract_attention_mask,\n",
                "    val_abstract_attention_mask,\n",
                ") = train_test_split(\n",
                "    abstract_inputs[\"input_ids\"],\n",
                "    abstract_attention_mask,\n",
                "    test_size=0.2,\n",
                "    random_state=42,\n",
                ")\n",
                "\n",
                "(\n",
                "    train_keywords_inputs,\n",
                "    val_keywords_inputs,\n",
                "    train_keywords_attention_mask,\n",
                "    val_keywords_attention_mask,\n",
                ") = train_test_split(\n",
                "    keywords_inputs[\"input_ids\"],\n",
                "    keywords_attention_mask,\n",
                "    test_size=0.2,\n",
                "    random_state=42,\n",
                ")\n",
                "\n",
                "# Create DataLoader for training and validation sets for each input type\n",
                "train_title_dataset = TensorDataset(\n",
                "    train_title_inputs, train_title_attention_mask, train_labels\n",
                ")\n",
                "val_title_dataset = TensorDataset(\n",
                "    val_title_inputs, val_title_attention_mask, val_labels\n",
                ")\n",
                "\n",
                "train_abstract_dataset = TensorDataset(\n",
                "    train_abstract_inputs, train_abstract_attention_mask, train_labels\n",
                ")\n",
                "val_abstract_dataset = TensorDataset(\n",
                "    val_abstract_inputs, val_abstract_attention_mask, val_labels\n",
                ")\n",
                "\n",
                "train_keywords_dataset = TensorDataset(\n",
                "    train_keywords_inputs, train_keywords_attention_mask, train_labels\n",
                ")\n",
                "val_keywords_dataset = TensorDataset(\n",
                "    val_keywords_inputs, val_keywords_attention_mask, val_labels\n",
                ")\n",
                "\n",
                "train_title_dataloader = DataLoader(train_title_dataset, batch_size=16, shuffle=True)\n",
                "val_title_dataloader = DataLoader(val_title_dataset, batch_size=16)\n",
                "\n",
                "train_abstract_dataloader = DataLoader(\n",
                "    train_abstract_dataset, batch_size=16, shuffle=True\n",
                ")\n",
                "val_abstract_dataloader = DataLoader(val_abstract_dataset, batch_size=16)\n",
                "\n",
                "train_keywords_dataloader = DataLoader(\n",
                "    train_keywords_dataset, batch_size=16, shuffle=True\n",
                ")\n",
                "val_keywords_dataloader = DataLoader(val_keywords_dataset, batch_size=16)\n",
                "\n",
                "# Define the classification head (unchanged)\n",
                "classification_head = nn.Sequential(\n",
                "    nn.Linear(\n",
                "        model.config.hidden_size * 3, 64\n",
                "    ),  # Concatenate the three outputs from title, abstract, and keywords\n",
                "    nn.ReLU(),\n",
                "    nn.Dropout(0.2),\n",
                "    nn.Linear(64, 2),  # 2 for binary classification (included/excluded)\n",
                ")\n",
                "\n",
                "# Fine-tune the model (unchanged)\n",
                "optimizer = torch.optim.AdamW(classification_head.parameters(), lr=2e-5)\n",
                "loss_fn = nn.CrossEntropyLoss()\n",
                "\n",
                "# Training loop (unchanged)\n",
                "num_epochs = 5 \n",
                "for epoch in range(num_epochs):\n",
                "    model.train()\n",
                "\n",
                "    for batch_title, batch_abstract, batch_keywords, batch_labels in zip(\n",
                "        train_title_dataloader,\n",
                "        train_abstract_dataloader,\n",
                "        train_keywords_dataloader,\n",
                "        train_labels,\n",
                "    ):\n",
                "        # Unpack the data for each input type\n",
                "        title_input_ids, title_attention_mask, _ = batch_title\n",
                "        abstract_input_ids, abstract_attention_mask, _ = batch_abstract\n",
                "        keywords_input_ids, keywords_attention_mask, _ = batch_keywords\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "\n",
                "        with torch.no_grad():\n",
                "            title_outputs = model(\n",
                "                input_ids=title_input_ids, attention_mask=title_attention_mask\n",
                "            )\n",
                "            abstract_outputs = model(\n",
                "                input_ids=abstract_input_ids, attention_mask=abstract_attention_mask\n",
                "            )\n",
                "            keywords_outputs = model(\n",
                "                input_ids=keywords_input_ids, attention_mask=keywords_attention_mask\n",
                "            )\n",
                "\n",
                "        # Concatenate the outputs from title, abstract, and keywords\n",
                "        concatenated_output = torch.cat(\n",
                "            [\n",
                "                title_outputs.last_hidden_state[:, 0, :],\n",
                "                abstract_outputs.last_hidden_state[:, 0, :],\n",
                "                keywords_outputs.last_hidden_state[:, 0, :],\n",
                "            ],\n",
                "            dim=1,\n",
                "        )\n",
                "\n",
                "        logits = classification_head(concatenated_output)\n",
                "        loss = loss_fn(logits, batch_labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "# Validation (updated)\n",
                "model.eval()\n",
                "val_loss = 0.0\n",
                "correct_predictions = 0\n",
                "total_predictions = 0\n",
                "\n",
                "# Check if GPU is available and set the device\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "with torch.no_grad():\n",
                "    for batch_title, batch_abstract, batch_keywords, batch_labels in zip(\n",
                "        val_title_dataloader,\n",
                "        val_abstract_dataloader,\n",
                "        val_keywords_dataloader,\n",
                "        val_labels,\n",
                "    ):\n",
                "        # Unpack the data for each input type\n",
                "        title_input_ids, title_attention_mask, _ = batch_title\n",
                "        abstract_input_ids, abstract_attention_mask, _ = batch_abstract\n",
                "        keywords_input_ids, keywords_attention_mask, _ = batch_keywords\n",
                "        batch_labels = batch_labels.to(device)  # Send labels to the device\n",
                "\n",
                "        title_outputs = model(\n",
                "            input_ids=title_input_ids.to(device),\n",
                "            attention_mask=title_attention_mask.to(device),\n",
                "        )\n",
                "        abstract_outputs = model(\n",
                "            input_ids=abstract_input_ids.to(device),\n",
                "            attention_mask=abstract_attention_mask.to(device),\n",
                "        )\n",
                "        keywords_outputs = model(\n",
                "            input_ids=keywords_input_ids.to(device),\n",
                "            attention_mask=keywords_attention_mask.to(device),\n",
                "        )\n",
                "\n",
                "        # Concatenate the outputs from title, abstract, and keywords\n",
                "        concatenated_output = torch.cat(\n",
                "            [\n",
                "                title_outputs.last_hidden_state[:, 0, :],\n",
                "                abstract_outputs.last_hidden_state[:, 0, :],\n",
                "                keywords_outputs.last_hidden_state[:, 0, :],\n",
                "            ],\n",
                "            dim=1,\n",
                "        )\n",
                "\n",
                "        logits = classification_head(concatenated_output)\n",
                "        loss = loss_fn(logits, batch_labels)\n",
                "        val_loss += loss.item()\n",
                "\n",
                "        _, predicted = torch.max(logits, 1)\n",
                "        correct_predictions += (predicted == batch_labels).sum().item()\n",
                "        total_predictions += batch_labels.size(0)\n",
                "\n",
                "avg_val_loss = val_loss / len(val_title_dataloader)\n",
                "accuracy = correct_predictions / total_predictions\n",
                "\n",
                "print(\n",
                "    f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\"\n",
                ")\n",
                "\n",
                "# Save the fine-tuned model (unchanged)\n",
                "torch.save(classification_head.state_dict(), \"fine_tuned_model.pt\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ds",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}