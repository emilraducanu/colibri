{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "DistilBert is a smaller version of BERT that is much faster and cheaper.\n",
                "\n",
                "From the paper,\n",
                "\n",
                ">\"we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster\"\n",
                "\n",
                "DistilBert Paper: https://arxiv.org/abs/1910.01108v4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
                        "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
                        "                                                                       \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/5 - Validation F1-score: 0.4284\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                               \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2/5 - Validation F1-score: 0.4284\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                       \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3/5 - Validation F1-score: 0.4284\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                       \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 4/5 - Validation F1-score: 0.4284\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                       \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 5/5 - Validation F1-score: 0.4284\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from transformers import DistilBertTokenizer, DistilBertModel\n",
                "from sklearn.model_selection import train_test_split\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "from tqdm import tqdm\n",
                "from sklearn.metrics import f1_score\n",
                "\n",
                "# Load pre-trained DistilBERT model and tokenizer\n",
                "model_name = \"distilbert-base-uncased\"\n",
                "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
                "model = DistilBertModel.from_pretrained(model_name)\n",
                "\n",
                "# Convert columns to list\n",
                "df = pd.read_pickle(\"/home/er/Documents/Cirad/colibri/data/trainset/trainset.pkl\")\n",
                "titles = df[\"Title\"].tolist()\n",
                "abstracts = df[\"Abstract\"].tolist()\n",
                "keywords = df[\"Keywords\"].tolist()\n",
                "\n",
                "# Tokenize and encode the titles, abstracts, and keywords\n",
                "titles = tokenizer(\n",
                "    titles, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\"\n",
                ")\n",
                "abstracts = tokenizer(\n",
                "    abstracts,\n",
                "    padding=\"max_length\",\n",
                "    truncation=True,\n",
                "    max_length=128,\n",
                "    return_tensors=\"pt\",\n",
                ")\n",
                "keywords = tokenizer(\n",
                "    keywords, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\"\n",
                ")\n",
                "\n",
                "# Create attention masks for each input\n",
                "titles_attention_masks = titles[\"attention_mask\"]\n",
                "abstracts_attention_masks = abstracts[\"attention_mask\"]\n",
                "keywords_attention_masks = keywords[\"attention_mask\"]\n",
                "\n",
                "# Prepare labels (assuming 'Screening' column has 'included' and 'excluded' values)\n",
                "labels = torch.tensor(df[\"Screening\"].map({\"included\": 1, \"excluded\": 0}).values)\n",
                "\n",
                "# Split data into training and validation sets\n",
                "(\n",
                "    train_titles,\n",
                "    val_titles,\n",
                "    train_titles_attention_mask,\n",
                "    val_titles_attention_mask,\n",
                "    train_labels,\n",
                "    val_labels,\n",
                ") = train_test_split(\n",
                "    titles[\"input_ids\"],\n",
                "    titles_attention_masks,\n",
                "    labels,\n",
                "    test_size=0.2,\n",
                "    random_state=42,\n",
                ")\n",
                "\n",
                "(\n",
                "    train_abstracts,\n",
                "    val_abstracts,\n",
                "    train_abstracts_attention_mask,\n",
                "    val_abstracts_attention_mask,\n",
                ") = train_test_split(\n",
                "    abstracts[\"input_ids\"],\n",
                "    abstracts_attention_masks,\n",
                "    test_size=0.2,\n",
                "    random_state=42,\n",
                ")\n",
                "\n",
                "(\n",
                "    train_keywords,\n",
                "    val_keywords,\n",
                "    train_keywords_attention_mask,\n",
                "    val_keywords_attention_mask,\n",
                ") = train_test_split(\n",
                "    keywords[\"input_ids\"],\n",
                "    keywords_attention_masks,\n",
                "    test_size=0.2,\n",
                "    random_state=42,\n",
                ")\n",
                "\n",
                "# Create DataLoader for training and validation sets for each input type\n",
                "train_titles_dataset = TensorDataset(\n",
                "    train_titles, train_titles_attention_mask, train_labels\n",
                ")\n",
                "val_titles_dataset = TensorDataset(val_titles, val_titles_attention_mask, val_labels)\n",
                "\n",
                "train_abstracts_dataset = TensorDataset(\n",
                "    train_abstracts, train_abstracts_attention_mask, train_labels\n",
                ")\n",
                "val_abstracts_dataset = TensorDataset(\n",
                "    val_abstracts, val_abstracts_attention_mask, val_labels\n",
                ")\n",
                "\n",
                "train_keywords_dataset = TensorDataset(\n",
                "    train_keywords, train_keywords_attention_mask, train_labels\n",
                ")\n",
                "val_keywords_dataset = TensorDataset(\n",
                "    val_keywords, val_keywords_attention_mask, val_labels\n",
                ")\n",
                "\n",
                "train_titles_dataloader = DataLoader(train_titles_dataset, batch_size=16, shuffle=True)\n",
                "val_titles_dataloader = DataLoader(val_titles_dataset, batch_size=16)\n",
                "\n",
                "train_abstracts_dataloader = DataLoader(\n",
                "    train_abstracts_dataset, batch_size=16, shuffle=True\n",
                ")\n",
                "val_abstracts_dataloader = DataLoader(val_abstracts_dataset, batch_size=16)\n",
                "\n",
                "train_keywords_dataloader = DataLoader(\n",
                "    train_keywords_dataset, batch_size=16, shuffle=True\n",
                ")\n",
                "val_keywords_dataloader = DataLoader(val_keywords_dataset, batch_size=16)\n",
                "\n",
                "# Define the classification heads\n",
                "titles_classification_head = nn.Sequential(\n",
                "    nn.Linear(model.config.hidden_size, 64),\n",
                "    nn.ReLU(),\n",
                "    nn.Dropout(0.2),\n",
                "    nn.Linear(64, 2),\n",
                ")\n",
                "\n",
                "abstracts_classification_head = nn.Sequential(\n",
                "    nn.Linear(model.config.hidden_size, 64),\n",
                "    nn.ReLU(),\n",
                "    nn.Dropout(0.2),\n",
                "    nn.Linear(64, 2),\n",
                ")\n",
                "\n",
                "keywords_classification_head = nn.Sequential(\n",
                "    nn.Linear(model.config.hidden_size, 64),\n",
                "    nn.ReLU(),\n",
                "    nn.Dropout(0.2),\n",
                "    nn.Linear(64, 2),\n",
                ")\n",
                "\n",
                "# Fine-tune the model\n",
                "optimizer = torch.optim.AdamW(\n",
                "    list(titles_classification_head.parameters())\n",
                "    + list(abstracts_classification_head.parameters())\n",
                "    + list(keywords_classification_head.parameters()),\n",
                "    lr=2e-5,\n",
                ")\n",
                "\n",
                "loss_fn = nn.CrossEntropyLoss()\n",
                "\n",
                "# Define the optimizers for each classification head\n",
                "optimizer_titles = torch.optim.AdamW(titles_classification_head.parameters(), lr=2e-5)\n",
                "optimizer_abstracts = torch.optim.AdamW(\n",
                "    abstracts_classification_head.parameters(), lr=2e-5\n",
                ")\n",
                "optimizer_keywords = torch.optim.AdamW(\n",
                "    keywords_classification_head.parameters(), lr=2e-5\n",
                ")\n",
                "\n",
                "# Training loop\n",
                "num_epochs = 5\n",
                "for epoch in range(num_epochs):\n",
                "    model.train()\n",
                "    train_loss = 0.0\n",
                "\n",
                "    # Create a tqdm progress bar for the training loop\n",
                "    train_progress_bar = tqdm(\n",
                "        zip(\n",
                "            train_titles_dataloader,\n",
                "            train_abstracts_dataloader,\n",
                "            train_keywords_dataloader,\n",
                "            train_labels,\n",
                "        ),\n",
                "        total=len(train_titles_dataloader),\n",
                "        desc=f\"Epoch {epoch + 1}/{num_epochs} (Training)\",\n",
                "        leave=False,\n",
                "    )\n",
                "\n",
                "    for (\n",
                "        batch_titles,\n",
                "        batch_abstracts,\n",
                "        batch_keywords,\n",
                "        batch_labels,\n",
                "    ) in train_progress_bar:\n",
                "        batch_titles, batch_titles_attention_mask, batch_labels_titles = batch_titles\n",
                "        (\n",
                "            batch_abstracts,\n",
                "            batch_abstracts_attention_mask,\n",
                "            batch_labels_abstracts,\n",
                "        ) = batch_abstracts\n",
                "        (\n",
                "            batch_keywords,\n",
                "            batch_keywords_attention_mask,\n",
                "            batch_labels_keywords,\n",
                "        ) = batch_keywords\n",
                "\n",
                "        optimizer_titles.zero_grad()\n",
                "        optimizer_abstracts.zero_grad()\n",
                "        optimizer_keywords.zero_grad()\n",
                "\n",
                "        with torch.no_grad():\n",
                "            titles_outputs = model(\n",
                "                input_ids=batch_titles, attention_mask=batch_titles_attention_mask\n",
                "            )\n",
                "            abstracts_outputs = model(\n",
                "                input_ids=batch_abstracts, attention_mask=batch_abstracts_attention_mask\n",
                "            )\n",
                "            keywords_outputs = model(\n",
                "                input_ids=batch_keywords, attention_mask=batch_keywords_attention_mask\n",
                "            )\n",
                "\n",
                "        titles_logits = titles_classification_head(\n",
                "            titles_outputs.last_hidden_state[:, 0, :]\n",
                "        )\n",
                "        abstracts_logits = abstracts_classification_head(\n",
                "            abstracts_outputs.last_hidden_state[:, 0, :]\n",
                "        )\n",
                "        keywords_logits = keywords_classification_head(\n",
                "            keywords_outputs.last_hidden_state[:, 0, :]\n",
                "        )\n",
                "\n",
                "        combined_logits = titles_logits + abstracts_logits + keywords_logits\n",
                "\n",
                "        loss_titles = loss_fn(titles_logits, batch_labels_titles)\n",
                "        loss_abstracts = loss_fn(abstracts_logits, batch_labels_abstracts)\n",
                "        loss_keywords = loss_fn(keywords_logits, batch_labels_keywords)\n",
                "        loss = loss_titles + loss_abstracts + loss_keywords\n",
                "        loss.backward()\n",
                "\n",
                "        optimizer_titles.step()\n",
                "        optimizer_abstracts.step()\n",
                "        optimizer_keywords.step()\n",
                "\n",
                "        train_loss += loss.item()\n",
                "\n",
                "    # Update the tqdm progress bar description with average loss\n",
                "    train_progress_bar.set_postfix(\n",
                "        train_loss=f\"{train_loss / len(train_titles_dataloader):.4f}\"\n",
                "    )\n",
                "\n",
                "    # Validation loop\n",
                "    model.eval()\n",
                "    val_loss = 0.0\n",
                "    f1_scores = []\n",
                "\n",
                "    # Create a tqdm progress bar for the validation loop\n",
                "    val_progress_bar = tqdm(\n",
                "        zip(\n",
                "            val_titles_dataloader,\n",
                "            val_abstracts_dataloader,\n",
                "            val_keywords_dataloader,\n",
                "            val_labels,\n",
                "        ),\n",
                "        total=len(val_titles_dataloader),\n",
                "        desc=f\"Epoch {epoch + 1}/{num_epochs} (Validation)\",\n",
                "        leave=False,\n",
                "    )\n",
                "\n",
                "    # Check if GPU is available and set the device\n",
                "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "    with torch.no_grad():\n",
                "        for (\n",
                "            batch_titles,\n",
                "            batch_abstracts,\n",
                "            batch_keywords,\n",
                "            batch_labels,\n",
                "        ) in val_progress_bar:\n",
                "            # Unpack the data for each input type\n",
                "            titles_input_ids, titles_attention_mask, batch_labels_titles = batch_titles\n",
                "            (\n",
                "                abstracts_input_ids,\n",
                "                abstracts_attention_mask,\n",
                "                batch_labels_abstracts,\n",
                "            ) = batch_abstracts\n",
                "            (\n",
                "                keywords_input_ids,\n",
                "                keywords_attention_mask,\n",
                "                batch_labels_keywords,\n",
                "            ) = batch_keywords\n",
                "\n",
                "            # Send labels to the device\n",
                "            batch_labels_titles = batch_labels_titles.to(device)\n",
                "            batch_labels_abstracts = batch_labels_abstracts.to(device)\n",
                "            batch_labels_keywords = batch_labels_keywords.to(device)\n",
                "\n",
                "            titles_outputs = model(\n",
                "                input_ids=titles_input_ids.to(device),\n",
                "                attention_mask=titles_attention_mask.to(device),\n",
                "            )\n",
                "            abstracts_outputs = model(\n",
                "                input_ids=abstracts_input_ids.to(device),\n",
                "                attention_mask=abstracts_attention_mask.to(device),\n",
                "            )\n",
                "            keywords_outputs = model(\n",
                "                input_ids=keywords_input_ids.to(device),\n",
                "                attention_mask=keywords_attention_mask.to(device),\n",
                "            )\n",
                "\n",
                "            titles_logits = titles_classification_head(\n",
                "                titles_outputs.last_hidden_state[:, 0, :]\n",
                "            )\n",
                "            abstracts_logits = abstracts_classification_head(\n",
                "                abstracts_outputs.last_hidden_state[:, 0, :]\n",
                "            )\n",
                "            keywords_logits = keywords_classification_head(\n",
                "                keywords_outputs.last_hidden_state[:, 0, :]\n",
                "            )\n",
                "\n",
                "            # Calculate the overall logits by combining the logits from different input types\n",
                "            combined_logits = (titles_logits + abstracts_logits + keywords_logits) / 3\n",
                "\n",
                "            # Convert predictions to NumPy array\n",
                "            _, predicted = torch.max(combined_logits, 1)\n",
                "            predicted = predicted.cpu().numpy()\n",
                "\n",
                "            # Calculate F1-score for each batch\n",
                "            f1 = f1_score(\n",
                "                batch_labels_titles.cpu(), predicted, average=\"macro\", zero_division=1\n",
                "            )\n",
                "            f1_scores.append(f1)\n",
                "\n",
                "            # Calculate the loss for each input type\n",
                "            loss_titles = loss_fn(titles_logits, batch_labels_titles)\n",
                "            loss_abstracts = loss_fn(abstracts_logits, batch_labels_abstracts)\n",
                "            loss_keywords = loss_fn(keywords_logits, batch_labels_keywords)\n",
                "\n",
                "            # Calculate the total loss as a combination of losses from different input types\n",
                "            loss = loss_titles + loss_abstracts + loss_keywords\n",
                "            val_loss += loss.item()\n",
                "\n",
                "    avg_val_loss = val_loss / len(val_titles_dataloader)\n",
                "    avg_f1 = np.mean(f1_scores)\n",
                "\n",
                "    # Update the tqdm progress bar description with validation results\n",
                "    val_progress_bar.set_postfix(\n",
                "        val_loss=f\"{avg_val_loss:.4f}\", f1_score=f\"{avg_f1:.4f}\"\n",
                "    )\n",
                "\n",
                "    # Print the F1-score at the end of each epoch\n",
                "    print(f\"Epoch {epoch + 1}/{num_epochs} - Validation F1-score: {avg_f1:.4f}\")\n",
                "\n",
                "# Save the fine-tuned model\n",
                "torch.save(model.state_dict(), \"fine_tuned_model.pt\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ds",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
