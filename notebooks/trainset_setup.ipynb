{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Text Cleaning**\\\n",
    "Remove any unnecessary characters, such as special characters, punctuation, or HTML tags.\n",
    "Convert the text to lowercase to achieve case-insensitive matching.\n",
    "Remove any extra whitespace or leading/trailing spaces.\n",
    "2. **Tokenization**\\\n",
    "Split the text into individual words or tokens, as it helps the model understand the semantic meaning of each word.\n",
    "3. **Stopword Removal**\\\n",
    "Remove common words, known as stopwords (e.g., \"the,\" \"is,\" \"and\"), which may not contribute much to the classification task.\n",
    "You can use pre-defined stopword lists from libraries like NLTK or spaCy or create a custom list based on your specific domain.\n",
    "4. **Lemmatization or Stemming**\\\n",
    "Reduce words to their base or root form to normalize the text and reduce vocabulary size.\n",
    "Lemmatization aims to convert words to their base form (lemma) using linguistic rules.\n",
    "Stemming reduces words to their root form using simple heuristic algorithms.\n",
    "5. **Handling Abbreviations and Acronyms**\\\n",
    "Decide whether to expand or keep abbreviations and acronyms as they are, based on their relevance to the classification task.\n",
    "6. **Handling Numeric Data**\\\n",
    "Decide whether to replace numbers with a generic token or keep them as-is based on their importance in the text.\n",
    "7. **Handling Rare Words or Outliers**\\\n",
    "Remove extremely rare words that occur infrequently, as they may not contribute significantly to the classification task.\n",
    "Similarly, remove any outliers or unusual words that may not be relevant to the task.\n",
    "8. **Vectorization**\\\n",
    "Convert the pre-processed text data into numerical representations that machine learning models can understand.\n",
    "Techniques such as TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings like Word2Vec or GloVe can be employed for vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/er/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/er/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/er/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw database contains 1007 entries (217 included MA and 790 excluded MA), stored into 'raw_data' variable.\n"
     ]
    }
   ],
   "source": [
    "# ------- LOADING DATA INTO A DATAFRAME -------\n",
    "\n",
    "\n",
    "# Loading database created by D. Beillouin et al.\n",
    "xlsx_file = pd.ExcelFile(\n",
    "    \"/home/er/Documents/Cirad/SOCSciCompiler/data/trainset/trainset.xlsx\"\n",
    ")\n",
    "\n",
    "# Adding label \"excluded\" or \"included\" for each MA\n",
    "df_incl = xlsx_file.parse(\"retained_meta-analyses\")\n",
    "df_excl = xlsx_file.parse(\"non_retained_meta-analyses\")\n",
    "df_incl[\"Screening\"] = \"included\"\n",
    "df_excl[\"Screening\"] = \"excluded\"\n",
    "\n",
    "# Keeping only useful attributes\n",
    "attributes_to_keep_incl = [\n",
    "    \"Screening\",\n",
    "    \"link\",\n",
    "    \"Article Title\",\n",
    "    \"Abstract\",\n",
    "    \"Keywords\",\n",
    "]\n",
    "attributes_to_keep_excl = [\n",
    "    \"Screening\",\n",
    "    \"lien pour accès\",\n",
    "    \"title\",\n",
    "    \"Abstract\",\n",
    "    \"Keywords\",\n",
    "]\n",
    "df_incl = df_incl[attributes_to_keep_incl]\n",
    "df_excl = df_excl[attributes_to_keep_excl]\n",
    "\n",
    "# Standardising columns names\n",
    "new_column_names_incl = {\"Article Title\": \"Title\", \"link\": \"DOI\"}\n",
    "new_column_names_excl = {\"title\": \"Title\", \"lien pour accès\": \"DOI\"}\n",
    "df_incl = df_incl.rename(columns=new_column_names_incl)\n",
    "df_excl = df_excl.rename(columns=new_column_names_excl)\n",
    "\n",
    "# Merging exluded and included MA into single dataframe\n",
    "raw_data = pd.concat([df_incl, df_excl], ignore_index=True)\n",
    "raw_data = raw_data.fillna(\"\")\n",
    "\n",
    "size_1 = len(df_incl)\n",
    "size_2 = len(df_excl)\n",
    "size_3 = size_1 + size_2\n",
    "print(\n",
    "    f\"Raw database contains {size_3} entries ({size_1} included MA and {size_2} excluded MA), stored into 'raw_data' variable.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 rows removed because of empty DOIs. Cannot check the uniqueness.\n",
      "0 rows removed because of empty titles. Cannot be processed by the ML model.\n",
      "54 rows removed because of empty abstracts. Cannot be processed by the ML model.\n",
      "8 DOI duplicates and title duplicates removed.\n",
      "Cleaned database contains 794 entries (212 included MA and 582 excluded MA), stored into 'train_set' variable.\n"
     ]
    }
   ],
   "source": [
    "# ------- CLEANING -------\n",
    "\n",
    "\n",
    "# Function to get DOIs from URLs\n",
    "def extract_doi(url):\n",
    "    if str(url).startswith(\"https://doi.org/\"):\n",
    "        return str(url)[len(\"https://doi.org/\") :]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Extracting DOIs from URLs\n",
    "raw_data[\"DOI\"] = raw_data[\"DOI\"].apply(extract_doi)\n",
    "\n",
    "# Removing empty DOIs rows\n",
    "raw_data = raw_data.dropna(subset=[\"DOI\"])\n",
    "size_4 = len(raw_data)\n",
    "size_5 = size_3 - size_4\n",
    "print(f\"{size_5} rows removed because of empty DOIs. Cannot check the uniqueness.\")\n",
    "\n",
    "# Removing empty titles rows\n",
    "raw_data[\"Title\"] = raw_data[\"Title\"].replace(\"\", np.nan)\n",
    "raw_data = raw_data.dropna(subset=[\"Title\"])\n",
    "size_6 = len(raw_data)\n",
    "size_7 = size_4 - size_6\n",
    "print(\n",
    "    f\"{size_7} rows removed because of empty titles. Cannot be processed by the ML model.\"\n",
    ")\n",
    "\n",
    "# Removing empty abstracts rows\n",
    "raw_data[\"Abstract\"] = raw_data[\"Abstract\"].replace(\"\", np.nan)\n",
    "raw_data = raw_data.dropna(subset=[\"Abstract\"])\n",
    "size_8 = len(raw_data)\n",
    "size_9 = size_6 - size_8\n",
    "print(\n",
    "    f\"{size_9} rows removed because of empty abstracts. Cannot be processed by the ML model.\"\n",
    ")\n",
    "\n",
    "# Removing DOIs duplicates and titles duplicates\n",
    "raw_data = raw_data.drop_duplicates(subset=[\"DOI\"], keep=\"first\")\n",
    "raw_data = raw_data.drop_duplicates(subset=\"Title\", keep=\"first\")\n",
    "size_10 = len(raw_data)\n",
    "size_11 = size_8 - size_10\n",
    "print(f\"{size_11} DOI duplicates and title duplicates removed.\")\n",
    "\n",
    "# Droping column 'DOI' now we have unique values. No needed for the ML model\n",
    "raw_data = raw_data.drop(columns=[\"DOI\"])\n",
    "\n",
    "size_12 = raw_data[\"Screening\"].value_counts()\n",
    "size_incl = size_12.loc[\"included\"]\n",
    "size_excl = size_12.loc[\"excluded\"]\n",
    "print(\n",
    "    f\"Cleaned database contains {size_10} entries ({size_incl} included MA and {size_excl} excluded MA), stored into 'train_set' variable.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 rows removed because of empty DOIs. Cannot check the uniqueness.\n",
      "0 rows removed because of empty titles. Cannot be processed by the ML model.\n",
      "54 rows removed because of empty abstracts. Cannot be processed by the ML model.\n",
      "8 DOI duplicates and title duplicates removed.\n"
     ]
    }
   ],
   "source": [
    "# ------- PRE-PROCESSING -------\n",
    "\n",
    "\n",
    "# for title in train_set[\"Title\"]\n",
    "\n",
    "# cleaned_titles = []\n",
    "# cleaned_abstracts = []\n",
    "# cleaned_keywords = []\n",
    "# tmp_keywords = []\n",
    "\n",
    "# for title, abstract, keywords in zip(\n",
    "#     train_set[\"Title\"], train_set[\"Abstract\"], train_set[\"Keywords\"]\n",
    "# ):\n",
    "#     title = re.sub(r\"[^\\w\\s]\", \"\", title)  # Remove special characters/punctuation\n",
    "#     title = title.lower()  # Convert to lowercase\n",
    "#     title = title.strip()  # Remove leading/trailing spaces\n",
    "#     cleaned_titles.append(title)\n",
    "\n",
    "#     abstract = re.sub(r\"[^\\w\\s]\", \"\", abstract)\n",
    "#     abstract = abstract.lower()\n",
    "#     abstract = abstract.strip()\n",
    "#     cleaned_abstracts.append(abstract)\n",
    "\n",
    "#     keywords = re.split(r\"[,;\\n]\", keywords)  # Split keywords using separators (, ; \\n)\n",
    "#     tmp_keywords.append(\n",
    "#         [keyword.strip() for keyword in keywords if keyword.strip() != \"\"]\n",
    "#     )\n",
    "#     for list_kw in tmp_keywords:\n",
    "#         tmp = []\n",
    "#         for keyword in list_kw:\n",
    "#             keyword = re.sub(r\"[^\\w\\s]\", \"\", keyword)\n",
    "#             keyword = keyword.lower()\n",
    "#             keyword = keyword.strip()\n",
    "#             tmp.append(keyword)\n",
    "#         cleaned_keywords.append(tmp)\n",
    "\n",
    "# # Tokenization, Stopword Removal, Lemmatization\n",
    "# tokenized_titles = []\n",
    "# tokenized_abstracts = []\n",
    "# tokenized_keywords = []\n",
    "\n",
    "# for title in cleaned_titles:\n",
    "#     tokens = nltk.word_tokenize(title)  # Tokenization\n",
    "#     filtered_tokens = [\n",
    "#         token for token in tokens if token not in stopwords\n",
    "#     ]  # Stopword Removal\n",
    "#     lemmas = [lemmatizer.lemmatize(token) for token in filtered_tokens]  # Lemmatization\n",
    "#     tokenized_titles.append(lemmas)\n",
    "\n",
    "# for abstract in cleaned_abstracts:\n",
    "#     tokens = nltk.word_tokenize(abstract)\n",
    "#     filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "#     lemmas = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "#     tokenized_abstracts.append(lemmas)\n",
    "\n",
    "# for keywords in cleaned_keywords:\n",
    "#     tokenized_keyword_list = []\n",
    "#     for keyword in keywords:\n",
    "#         tokens = nltk.word_tokenize(keyword)\n",
    "#         filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "#         lemmas = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "#         tokenized_keyword_list.extend(lemmas)\n",
    "#     tokenized_keywords.append(tokenized_keyword_list)\n",
    "\n",
    "# # Update the DataFrame with pre-processed data\n",
    "# train_set[\"Tokenized Titles\"] = tokenized_titles\n",
    "# train_set[\"Tokenized Abstracts\"] = tokenized_abstracts\n",
    "# train_set[\"Tokenized Keywords\"] = tokenized_keywords\n",
    "\n",
    "# rows_to_keep = [\n",
    "#     \"Screening\",\n",
    "#     \"Tokenized Titles\",\n",
    "#     \"Tokenized Abstracts\",\n",
    "#     \"Tokenized Keywords\",\n",
    "# ]\n",
    "\n",
    "# train_set = train_set[rows_to_keep]\n",
    "# train_set.columns = [\"Screening\", \"Title\", \"Abstract\", \"Keyword\"]\n",
    "\n",
    "# train_set = train_set.sample(frac=1)\n",
    "# train_set = train_set.drop(train_set[train_set[\"Abstract\"].apply(len) == 0].index)\n",
    "# train_set = train_set.reset_index(drop=True)\n",
    "\n",
    "# empty_list_count = 0\n",
    "\n",
    "# # Iterate over rows in the 'Abstract' column\n",
    "# for abstract_list in train_set[\"Keyword\"]:\n",
    "#     if abstract_list == []:\n",
    "#         empty_list_count += 1\n",
    "\n",
    "# print(\"Number of empty lists in 'Abstract' column:\", empty_list_count)\n",
    "\n",
    "# raw_data = raw_data.reset_index(drop=True)\n",
    "# raw_data.info()\n",
    "# raw_data.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
